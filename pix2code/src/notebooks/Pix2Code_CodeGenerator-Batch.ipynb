{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2767e1-744c-4c0f-a6f5-c9bd7d3c97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from os.path import basename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd255c51-ac12-4579-bae0-a56512bf5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imageprocessing_utils:\n",
    "    @staticmethod\n",
    "    def sparsify(label_vector, output_size):\n",
    "        sparse_vector = []\n",
    "\n",
    "        for label in label_vector:\n",
    "            sparse_label = np.zeros(output_size)\n",
    "            sparse_label[label] = 1\n",
    "\n",
    "            sparse_vector.append(sparse_label)\n",
    "\n",
    "        return np.array(sparse_vector)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_preprocessed_img(img_path, image_size):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        img = img.astype('float32')\n",
    "        img /= 255\n",
    "        return img\n",
    "\n",
    "    @staticmethod\n",
    "    def show(image):\n",
    "        cv2.namedWindow(\"view\", cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow(\"view\", image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"view\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657e2f5e-249d-4662-bb5f-a2c393194e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = 48\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 72000\n",
    "\n",
    "\n",
    "START_TOKEN = \"<START>\"\n",
    "END_TOKEN = \"<END>\"\n",
    "PLACEHOLDER = \" \"\n",
    "SEPARATOR = '->'\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.binary_vocabulary = {}\n",
    "        self.vocabulary = {}\n",
    "        self.token_lookup = {}\n",
    "        self.size = 0\n",
    "\n",
    "        self.append(START_TOKEN)\n",
    "        self.append(END_TOKEN)\n",
    "        self.append(PLACEHOLDER)\n",
    "\n",
    "    def append(self, token):\n",
    "        if token not in self.vocabulary:\n",
    "            self.vocabulary[token] = self.size\n",
    "            self.token_lookup[self.size] = token\n",
    "            self.size += 1\n",
    "\n",
    "    def create_binary_representation(self):\n",
    "        if sys.version_info >= (3,):\n",
    "            items = self.vocabulary.items()\n",
    "        else:\n",
    "            items = self.vocabulary.iteritems()\n",
    "        for key, value in items:\n",
    "            binary = np.zeros(self.size)\n",
    "            binary[value] = 1\n",
    "            self.binary_vocabulary[key] = binary\n",
    "\n",
    "    def get_serialized_binary_representation(self):\n",
    "        if len(self.binary_vocabulary) == 0:\n",
    "            self.create_binary_representation()\n",
    "\n",
    "        string = \"\"\n",
    "        if sys.version_info >= (3,):\n",
    "            items = self.binary_vocabulary.items()\n",
    "        else:\n",
    "            items = self.binary_vocabulary.iteritems()\n",
    "        for key, value in items:\n",
    "            array_as_string = np.array2string(value, separator=',', max_line_width=self.size * self.size)\n",
    "            string += \"{}{}{}\\n\".format(key, SEPARATOR, array_as_string[1:len(array_as_string) - 1])\n",
    "        return string\n",
    "\n",
    "    def save(self, path):\n",
    "        output_file_name = \"{}/words.vocab\".format(path)\n",
    "        output_file = open(output_file_name, 'w')\n",
    "        output_file.write(self.get_serialized_binary_representation())\n",
    "        output_file.close()\n",
    "\n",
    "    def retrieve(self, path):\n",
    "        input_file = open(\"{}/words.vocab\".format(path), 'r')\n",
    "        buffer = \"\"\n",
    "        for line in input_file:\n",
    "            try:\n",
    "                separator_position = len(buffer) + line.index(SEPARATOR)\n",
    "                buffer += line\n",
    "                key = buffer[:separator_position]\n",
    "                value = buffer[separator_position + len(SEPARATOR):]\n",
    "                value = np.fromstring(value, sep=',')\n",
    "\n",
    "                self.binary_vocabulary[key] = value\n",
    "                self.vocabulary[key] = np.where(value == 1)[0][0]\n",
    "                self.token_lookup[np.where(value == 1)[0][0]] = key\n",
    "\n",
    "                buffer = \"\"\n",
    "            except ValueError:\n",
    "                buffer += line\n",
    "        input_file.close()\n",
    "        self.size = len(self.vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfafc624-deab-4bca-8205-62f5223251bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "class AModel:\n",
    "    def __init__(self, input_shape, output_size, output_path):\n",
    "        self.model = None\n",
    "        self.input_shape = input_shape\n",
    "        self.output_size = output_size\n",
    "        self.output_path = output_path\n",
    "        self.name = \"\"\n",
    "\n",
    "    def save(self):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(\"{}/{}.json\".format(self.output_path, self.name), \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        self.model.save_weights(\"{}/{}.h5\".format(self.output_path, self.name))\n",
    "\n",
    "    def load(self, name=\"\"):\n",
    "        output_name = self.name if name == \"\" else name\n",
    "        with open(\"{}/{}.json\".format(self.output_path, output_name), \"r\") as json_file:\n",
    "            loaded_model_json = json_file.read()\n",
    "        self.model = model_from_json(loaded_model_json)\n",
    "        self.model.load_weights(\"{}/{}.h5\".format(self.output_path, output_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2994b9b-984e-4696-9d6b-2bcdeb04b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Input, Dense, Dropout, \\\n",
    "                         RepeatVector, LSTM, concatenate, \\\n",
    "                         Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers.legacy import RMSprop\n",
    "from keras import *\n",
    "\n",
    "class pix2code(AModel):\n",
    "    def __init__(self, input_shape, output_size, output_path):\n",
    "        AModel.__init__(self, input_shape, output_size, output_path)\n",
    "        self.name = \"pix2code\"\n",
    "\n",
    "        image_model = Sequential()\n",
    "        image_model.add(Conv2D(32, (3, 3), padding='valid', activation='relu', input_shape=input_shape))\n",
    "        image_model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        image_model.add(Dropout(0.25))\n",
    "\n",
    "        image_model.add(Conv2D(64, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(Conv2D(64, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        image_model.add(Dropout(0.25))\n",
    "\n",
    "        image_model.add(Conv2D(128, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(Conv2D(128, (3, 3), padding='valid', activation='relu'))\n",
    "        image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        image_model.add(Dropout(0.25))\n",
    "\n",
    "        image_model.add(Flatten())\n",
    "        image_model.add(Dense(1024, activation='relu'))\n",
    "        image_model.add(Dropout(0.3))\n",
    "        image_model.add(Dense(1024, activation='relu'))\n",
    "        image_model.add(Dropout(0.3))\n",
    "\n",
    "        image_model.add(RepeatVector(CONTEXT_LENGTH))\n",
    "\n",
    "        visual_input = Input(shape=input_shape)\n",
    "        encoded_image = image_model(visual_input)\n",
    "\n",
    "        language_model = Sequential()\n",
    "        language_model.add(LSTM(128, return_sequences=True, input_shape=(CONTEXT_LENGTH, output_size)))\n",
    "        language_model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "        textual_input = Input(shape=(CONTEXT_LENGTH, output_size))\n",
    "        encoded_text = language_model(textual_input)\n",
    "\n",
    "        decoder = concatenate([encoded_image, encoded_text])\n",
    "\n",
    "        decoder = LSTM(512, return_sequences=True)(decoder)\n",
    "        decoder = LSTM(512, return_sequences=False)(decoder)\n",
    "        decoder = Dense(output_size, activation='softmax')(decoder)\n",
    "\n",
    "        self.model = Model(inputs=[visual_input, textual_input], outputs=decoder)\n",
    "\n",
    "        optimizer = RMSprop(learning_rate=0.0001, clipvalue=1.0)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def fit(self, images, partial_captions, next_words):\n",
    "        self.model.fit([images, partial_captions], next_words, shuffle=False, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
    "        self.save()\n",
    "\n",
    "    def fit_generator(self, generator, steps_per_epoch):\n",
    "        self.model.fit(generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, verbose=1)\n",
    "        self.save()\n",
    "\n",
    "    def predict(self, image, partial_caption):\n",
    "        return self.model.predict([image, partial_caption], verbose=0)[0]\n",
    "\n",
    "    def predict_batch(self, images, partial_captions):\n",
    "        return self.model.predict([images, partial_captions], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe5c0f4-8407-4f51-a286-ed45760c0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, key, value, data=None):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.data = data\n",
    "        self.parent = None\n",
    "        self.root = None\n",
    "        self.children = []\n",
    "        self.level = 0\n",
    "\n",
    "    def add_children(self, children, beam_width):\n",
    "        for child in children:\n",
    "            child.level = self.level + 1\n",
    "            child.value = child.value * self.value\n",
    "\n",
    "        nodes = sorted(children, key=lambda node: node.value, reverse=True)\n",
    "        nodes = nodes[:beam_width]\n",
    "\n",
    "        for node in nodes:\n",
    "            self.children.append(node)\n",
    "            node.parent = self\n",
    "\n",
    "        if self.parent is None:\n",
    "            self.root = self\n",
    "        else:\n",
    "            self.root = self.parent.root\n",
    "        child.root = self.root\n",
    "\n",
    "    def remove_child(self, child):\n",
    "        self.children.remove(child)\n",
    "\n",
    "    def max_child(self):\n",
    "        if len(self.children) == 0:\n",
    "            return self\n",
    "\n",
    "        max_childs = []\n",
    "        for child in self.children:\n",
    "            max_childs.append(child.max_child())\n",
    "\n",
    "        nodes = sorted(max_childs, key=lambda child: child.value, reverse=True)\n",
    "        return nodes[0]\n",
    "\n",
    "    def show(self, depth=0):\n",
    "        print(\" \" * depth, self.key, self.value, self.level)\n",
    "        for child in self.children:\n",
    "            child.show(depth + 2)\n",
    "\n",
    "\n",
    "class BeamSearch:\n",
    "    def __init__(self, beam_width=1):\n",
    "        self.beam_width = beam_width\n",
    "\n",
    "        self.root = None\n",
    "        self.clear()\n",
    "\n",
    "    def search(self):\n",
    "        result = self.root.max_child()\n",
    "\n",
    "        self.clear()\n",
    "        return self.retrieve_path(result)\n",
    "\n",
    "    def add_nodes(self, parent, children):\n",
    "        parent.add_children(children, self.beam_width)\n",
    "\n",
    "    def is_valid(self):\n",
    "        leaves = self.get_leaves()\n",
    "        level = leaves[0].level\n",
    "        counter = 0\n",
    "        for leaf in leaves:\n",
    "            if leaf.level == level:\n",
    "                counter += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if counter == len(leaves):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_leaves(self):\n",
    "        leaves = []\n",
    "        self.search_leaves(self.root, leaves)\n",
    "        return leaves\n",
    "\n",
    "    def search_leaves(self, node, leaves):\n",
    "        for child in node.children:\n",
    "            if len(child.children) == 0:\n",
    "                leaves.append(child)\n",
    "            else:\n",
    "                self.search_leaves(child, leaves)\n",
    "\n",
    "    def prune_leaves(self):\n",
    "        leaves = self.get_leaves()\n",
    "\n",
    "        nodes = sorted(leaves, key=lambda leaf: leaf.value, reverse=True)\n",
    "        nodes = nodes[self.beam_width:]\n",
    "\n",
    "        for node in nodes:\n",
    "            node.parent.remove_child(node)\n",
    "\n",
    "        while not self.is_valid():\n",
    "            leaves = self.get_leaves()\n",
    "            max_level = 0\n",
    "            for leaf in leaves:\n",
    "                if leaf.level > max_level:\n",
    "                    max_level = leaf.level\n",
    "\n",
    "            for leaf in leaves:\n",
    "                if leaf.level < max_level:\n",
    "                    leaf.parent.remove_child(leaf)\n",
    "\n",
    "    def clear(self):\n",
    "        self.root = None\n",
    "        self.root = Node(\"root\", 1.0, None)\n",
    "\n",
    "    def retrieve_path(self, end):\n",
    "        path = [end.key]\n",
    "        data = [end.data]\n",
    "        while end.parent is not None:\n",
    "            end = end.parent\n",
    "            path.append(end.key)\n",
    "            data.append(end.data)\n",
    "\n",
    "        result_path = []\n",
    "        result_data = []\n",
    "        for i in range(len(path) - 2, -1, -1):\n",
    "            result_path.append(path[i])\n",
    "            result_data.append(data[i])\n",
    "        return result_path, result_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b36344-0ef7-407c-bdeb-1563cf1c266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampler:\n",
    "    def __init__(self, voc_path, input_shape, output_size, context_length):\n",
    "        self.voc = Vocabulary()\n",
    "        self.voc.retrieve(voc_path)\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.output_size = output_size\n",
    "\n",
    "        print(\"Vocabulary size: {}\".format(self.voc.size))\n",
    "        print(\"Input shape: {}\".format(self.input_shape))\n",
    "        print(\"Output size: {}\".format(self.output_size))\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "    def predict_greedy(self, model, input_img, require_sparse_label=True, sequence_length=150, verbose=False):\n",
    "        current_context = [self.voc.vocabulary[PLACEHOLDER]] * (self.context_length - 1)\n",
    "        current_context.append(self.voc.vocabulary[START_TOKEN])\n",
    "        if require_sparse_label:\n",
    "            current_context = Imageprocessing_utils.sparsify(current_context, self.output_size)\n",
    "\n",
    "        predictions = START_TOKEN\n",
    "        out_probas = []\n",
    "\n",
    "        for i in range(0, sequence_length):\n",
    "            if verbose:\n",
    "                print(\"predicting {}/{}...\".format(i, sequence_length))\n",
    "\n",
    "            probas = model.predict(input_img, np.array([current_context]))\n",
    "            prediction = np.argmax(probas)\n",
    "            out_probas.append(probas)\n",
    "\n",
    "            new_context = []\n",
    "            for j in range(1, self.context_length):\n",
    "                new_context.append(current_context[j])\n",
    "\n",
    "            if require_sparse_label:\n",
    "                sparse_label = np.zeros(self.output_size)\n",
    "                sparse_label[prediction] = 1\n",
    "                new_context.append(sparse_label)\n",
    "            else:\n",
    "                new_context.append(prediction)\n",
    "\n",
    "            current_context = new_context\n",
    "\n",
    "            predictions += self.voc.token_lookup[prediction]\n",
    "\n",
    "            if self.voc.token_lookup[prediction] == END_TOKEN:\n",
    "                break\n",
    "\n",
    "        return predictions, out_probas\n",
    "\n",
    "    def recursive_beam_search(self, model, input_img, current_context, beam, current_node, sequence_length):\n",
    "        probas = model.predict(input_img, np.array([current_context]))\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(0, len(probas)):\n",
    "            predictions.append((i, probas[i], probas))\n",
    "\n",
    "        nodes = []\n",
    "        for i in range(0, len(predictions)):\n",
    "            prediction = predictions[i][0]\n",
    "            score = predictions[i][1]\n",
    "            output_probas = predictions[i][2]\n",
    "            nodes.append(Node(prediction, score, output_probas))\n",
    "\n",
    "        beam.add_nodes(current_node, nodes)\n",
    "\n",
    "        if beam.is_valid():\n",
    "            beam.prune_leaves()\n",
    "            if sequence_length == 1 or self.voc.token_lookup[beam.root.max_child().key] == END_TOKEN:\n",
    "                return\n",
    "\n",
    "            for node in beam.get_leaves():\n",
    "                prediction = node.key\n",
    "\n",
    "                new_context = []\n",
    "                for j in range(1, self.context_length):\n",
    "                    new_context.append(current_context[j])\n",
    "                sparse_label = np.zeros(self.output_size)\n",
    "                sparse_label[prediction] = 1\n",
    "                new_context.append(sparse_label)\n",
    "\n",
    "                self.recursive_beam_search(model, input_img, new_context, beam, node, sequence_length - 1)\n",
    "\n",
    "    def predict_beam_search(self, model, input_img, beam_width=3, require_sparse_label=True, sequence_length=150):\n",
    "        predictions = START_TOKEN\n",
    "        out_probas = []\n",
    "\n",
    "        current_context = [self.voc.vocabulary[PLACEHOLDER]] * (self.context_length - 1)\n",
    "        current_context.append(self.voc.vocabulary[START_TOKEN])\n",
    "        if require_sparse_label:\n",
    "            current_context = Imageprocessing_utils.sparsify(current_context, self.output_size)\n",
    "\n",
    "        beam = BeamSearch(beam_width=beam_width)\n",
    "\n",
    "        self.recursive_beam_search(model, input_img, current_context, beam, beam.root, sequence_length)\n",
    "\n",
    "        predicted_sequence, probas_sequence = beam.search()\n",
    "\n",
    "        for k in range(0, len(predicted_sequence)):\n",
    "            prediction = predicted_sequence[k]\n",
    "            probas = probas_sequence[k]\n",
    "            out_probas.append(probas)\n",
    "\n",
    "            predictions += self.voc.token_lookup[prediction]\n",
    "\n",
    "        return predictions, out_probas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0e4b04-edea-4c40-89f9-5357c82eacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CodeGenerator_utils:\n",
    "    @staticmethod\n",
    "    def get_random_text(length_text=10, space_number=1, with_upper_case=True):\n",
    "        results = []\n",
    "        while len(results) < length_text:\n",
    "            char = random.choice(string.ascii_letters[:26])\n",
    "            results.append(char)\n",
    "        if with_upper_case:\n",
    "            results[0] = results[0].upper()\n",
    "\n",
    "        current_spaces = []\n",
    "        while len(current_spaces) < space_number:\n",
    "            space_pos = random.randint(2, length_text - 3)\n",
    "            if space_pos in current_spaces:\n",
    "                break\n",
    "            results[space_pos] = \" \"\n",
    "            if with_upper_case:\n",
    "                results[space_pos + 1] = results[space_pos - 1].upper()\n",
    "\n",
    "            current_spaces.append(space_pos)\n",
    "\n",
    "        return ''.join(results)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_ios_id(length=10):\n",
    "        results = []\n",
    "\n",
    "        while len(results) < length:\n",
    "            char = random.choice(string.digits + string.ascii_letters)\n",
    "            results.append(char)\n",
    "\n",
    "        results[3] = \"-\"\n",
    "        results[6] = \"-\"\n",
    "\n",
    "        return ''.join(results)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_android_id(length=10):\n",
    "        results = []\n",
    "\n",
    "        while len(results) < length:\n",
    "            char = random.choice(string.ascii_letters)\n",
    "            results.append(char)\n",
    "\n",
    "        return ''.join(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d5f053-6cfa-4d84-8102-7065bf3f33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_Generator:\n",
    "    def __init__(self, key, parent_node, content_holder):\n",
    "        self.key = key\n",
    "        self.parent = parent_node\n",
    "        self.children = []\n",
    "        self.content_holder = content_holder\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "    def show(self):\n",
    "        # print(self.key)\n",
    "        for child in self.children:\n",
    "            child.show()\n",
    "\n",
    "    def render(self, mapping, rendering_function=None):\n",
    "        content = \"\"\n",
    "        for child in self.children:\n",
    "            content += child.render(mapping, rendering_function)\n",
    "\n",
    "        value = mapping[self.key]\n",
    "        if rendering_function is not None:\n",
    "            value = rendering_function(self.key, value)\n",
    "\n",
    "        if len(self.children) != 0:\n",
    "            value = value.replace(self.content_holder, content)\n",
    "\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75381bcc-2a20-480c-a141-9b8a9db6ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Compiler:\n",
    "    def __init__(self, dsl_mapping_file_path):\n",
    "        with open(dsl_mapping_file_path) as data_file:\n",
    "            self.dsl_mapping = json.load(data_file)\n",
    "\n",
    "        self.opening_tag = self.dsl_mapping[\"opening-tag\"]\n",
    "        self.closing_tag = self.dsl_mapping[\"closing-tag\"]\n",
    "        self.content_holder = self.opening_tag + self.closing_tag\n",
    "\n",
    "        # self.root = Node(\"body\", None, self.content_holder)\n",
    "\n",
    "    def compile(self, input_file_path, output_file_path, rendering_function=None):\n",
    "        self.root = Node_Generator(\"body\", None, self.content_holder)\n",
    "\n",
    "        dsl_file = open(input_file_path)\n",
    "        current_parent = self.root\n",
    "        for token in dsl_file:\n",
    "            token = token.replace(\" \", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "            if token.find(self.opening_tag) != -1:\n",
    "                token = token.replace(self.opening_tag, \"\")\n",
    "\n",
    "                element = Node_Generator(token, current_parent, self.content_holder)\n",
    "                current_parent.add_child(element)\n",
    "                current_parent = element\n",
    "            elif token.find(self.closing_tag) != -1:\n",
    "                current_parent = current_parent.parent\n",
    "            else:\n",
    "                tokens = token.split(\",\")\n",
    "                for t in tokens:\n",
    "                    element = Node_Generator(t, current_parent, self.content_holder)\n",
    "                    current_parent.add_child(element)\n",
    "\n",
    "        output_html = self.root.render(self.dsl_mapping, rendering_function=rendering_function)\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            output_file.write(output_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3335d2aa-7ecb-4552-8cb7-027428581347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 19\n",
      "Input shape: (256, 256, 3)\n",
      "Output size: 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To generate gui files\n",
    "\n",
    "model_web = \"/Users/abhjha8/Image2Code/pix2code/src/models/model_web\"\n",
    "# model_android = \"/Users/abhjha8/Image2Code/pix2code/src/models/model_android\"\n",
    "# model_ios = \"/Users/abhjha8/Image2Code/pix2code/src/models/model_ios\"\n",
    "\n",
    "trained_weights_path = \"/Users/abhjha8/Image2Code/pix2code/src/models/model_web\"\n",
    "trained_model_name = \"pix2code\"\n",
    "input_path = \"/Users/abhjha8/Image2Code/pix2code/src/dataset/web/eval_set\"\n",
    "gui_output_path = \"/Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch\"\n",
    "search_method = \"greedy\"\n",
    "\n",
    "if not os.path.exists(gui_output_path):\n",
    "    os.makedirs(gui_output_path)\n",
    "\n",
    "meta_dataset = np.load(\"{}/meta_dataset.npy\".format(trained_weights_path), allow_pickle=True)\n",
    "\n",
    "input_shape = meta_dataset[0]\n",
    "output_size = meta_dataset[1]\n",
    "\n",
    "model = pix2code(input_shape, output_size, trained_weights_path)\n",
    "model.load(trained_model_name)\n",
    "\n",
    "sampler = Sampler(trained_weights_path, input_shape, output_size, CONTEXT_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcd374b2-e7cc-40dd-b0d1-eb46ea5c4ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated GUI file count  1\n",
      "Generated GUI file count  2\n",
      "Generated GUI file count  3\n",
      "Generated GUI file count  4\n",
      "Generated GUI file count  5\n",
      "Generated GUI file count  6\n",
      "Generated GUI file count  7\n",
      "Generated GUI file count  8\n",
      "Generated GUI file count  9\n",
      "Generated GUI file count  10\n",
      "Generated GUI file count  11\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "total_count = 0;files_list = []\n",
    "for f in os.listdir(input_path):\n",
    "    if f.find(\".png\") != -1:\n",
    "        image_file_path = os.path.join(input_path,f)\n",
    "        total_count+=1\n",
    "    \n",
    "        evaluation_img = Imageprocessing_utils.get_preprocessed_img(image_file_path, IMAGE_SIZE)\n",
    "        \n",
    "        f_image = image_file_path.split(\"/\")[-1]\n",
    "        file_name = f_image[:f_image.find(\".png\")]\n",
    "    \n",
    "        files_list.append(file_name)\n",
    "        if search_method == \"greedy\":\n",
    "            result, _ = sampler.predict_greedy(model, np.array([evaluation_img]))\n",
    "            # print(\"Result greedy: {}\".format(result))\n",
    "        else:\n",
    "            beam_width = int(search_method)\n",
    "            # print(\"Search with beam width: {}\".format(beam_width))\n",
    "            result, _ = sampler.predict_beam_search(model, np.array([evaluation_img]), beam_width=beam_width)\n",
    "            # print(\"Result beam: {}\".format(result))\n",
    "        \n",
    "        with open(\"{}/{}.gui\".format(gui_output_path, file_name), 'w') as out_f:\n",
    "            out_f.write(result.replace(START_TOKEN, \"\").replace(END_TOKEN, \"\"))\n",
    "    \n",
    "        print(\"Generated GUI file count \",total_count)\n",
    "        if total_count>10:\n",
    "            break\n",
    "        \n",
    "print(len(files_list)==len(list(set(files_list))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a4cd91f-79fc-46ce-8663-66c75703c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_path = gui_output_path\n",
    "output_folder_path = \"/Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_final_test_batch/\"\n",
    "FILL_WITH_RANDOM_TEXT = True\n",
    "TEXT_PLACE_HOLDER = \"[]\"\n",
    "dsl_path = \"/Users/abhjha8/Image2Code/pix2code/src/utils/web-dsl-mapping.json\"\n",
    "compiler = Compiler(dsl_path)\n",
    "\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "def render_content_with_text(key, value):\n",
    "    if FILL_WITH_RANDOM_TEXT:\n",
    "        if key.find(\"btn\") != -1:\n",
    "            value = value.replace(TEXT_PLACE_HOLDER, CodeGenerator_utils.get_random_text())\n",
    "        elif key.find(\"title\") != -1:\n",
    "            value = value.replace(TEXT_PLACE_HOLDER, CodeGenerator_utils.get_random_text(length_text=5, space_number=0))\n",
    "        elif key.find(\"text\") != -1:\n",
    "            value = value.replace(TEXT_PLACE_HOLDER,\n",
    "                                  CodeGenerator_utils.get_random_text(length_text=56, space_number=7, with_upper_case=False))\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ae5353-0d3d-4f80-a5fb-b6a66addcda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not able to compile file: /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/2A63DBD3-FF02-4BA6-8062-BA617D941057.gui\n",
      "Exception raised ''\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/2BE940B3-F4A4-428F-86C8-10CB885CD62E.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/6F6151F8-FB4C-47C9-8FD2-532A19F45971.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/891C8311-6939-4665-B139-09AC75D0AAA4.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/AB660EA5-9546-4BD9-B2C6-00E53CBC2AE6.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/297E5EC4-4A0D-4B9E-837F-E5B9B1F84083.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/3C2FA230-DD5A-4182-8559-BD3F71943264.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/3CFAE645-5CD0-48F2-98C3-6DDBAA392C89.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/DB1A585C-D4C9-4362-AA4E-13CBE7B37067.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/FDBD8D36-B4F0-4398-9881-9B4EDE94BED1.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/65464850-0777-4513-9780-BC48F590FD2D.gui\n",
      "Compiled successfully  /Users/abhjha8/Image2Code/pix2code/src/dataset/web/model_result_gui_test_batch/30EEFC5A-7BAC-43BC-9DD0-F47A32F42195.gui\n",
      "Total files: 11 and exception count: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exception_count=0;total_count=0\n",
    "for input_file in os.listdir(input_folder_path):\n",
    "    if input_file.find(\".gui\")==-1:\n",
    "        continue\n",
    "    input_file = \"{}/{}\".format(input_folder_path, input_file)\n",
    "    \n",
    "    file_uid = basename(input_file)[:basename(input_file).find(\".\")]\n",
    "    path = input_file[:input_file.find(file_uid)]\n",
    "    input_file_path = \"{}{}.gui\".format(path, file_uid)\n",
    "    output_file_path = \"{}{}.html\".format(output_folder_path, file_uid)\n",
    "    \n",
    "    try:\n",
    "        compiler.compile(input_file_path, output_file_path, rendering_function=render_content_with_text)\n",
    "        print(\"Compiled successfully \",input_file_path)\n",
    "    except Exception as e:\n",
    "        print(\"Not able to compile file: {}\".format(input_file))\n",
    "        print(\"Exception raised {}\".format(e))\n",
    "        exception_count+=1\n",
    "        continue\n",
    "    total_count+=1\n",
    "\n",
    "print(\"Total files: {} and exception count: {}\".format(total_count, exception_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0685005-f75a-4c50-9ad7-455cc2d3e5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
